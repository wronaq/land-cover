{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598108739899",
   "display_name": "Python 3.7.3 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose, RandomChoice\n",
    "\n",
    "from utils import LandCoverDataset, Resize, ToTensor, Normalize, BrightnessJitter, ContrastJitter, SaturationJitter, HueJitter\n",
    "from models import UNet\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    DATA_FOLDER = 'data'\n",
    "    BATCH_SIZE = 16\n",
    "    TRAIN_SPLIT = .8\n",
    "    VAL_TEST_SPLIT = .5\n",
    "    SUFFLE_DATASET = True\n",
    "    RANDOM_SEED = 2137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mean and standard deviation for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Means: [107.20428231 115.20819438  91.24265174]\nStds:  [28.28229905 21.92473988 20.90197824]\n"
    }
   ],
   "source": [
    "# set train, valid and test indexes\n",
    "dataset = LandCoverDataset(root_dir=Config.DATA_FOLDER, transform=Resize((224,224)))\n",
    "indexes = list(range(len(dataset)))\n",
    "split_point = int(np.floor(Config.TRAIN_SPLIT * len(dataset)))\n",
    "if Config.SUFFLE_DATASET:\n",
    "    np.random.seed(Config.RANDOM_SEED)\n",
    "    np.random.shuffle(indexes)\n",
    "train_indexes, rest_indexes = indexes[:split_point], indexes[split_point:]\n",
    "val_test_split_point = int(np.floor(Config.VAL_TEST_SPLIT * len(rest_indexes)))\n",
    "valid_indexes, test_indexes = rest_indexes[:val_test_split_point], rest_indexes[val_test_split_point:]\n",
    "\n",
    "# make dataset samplers\n",
    "train_sampler = SubsetRandomSampler(train_indexes)\n",
    "valid_sampler = SubsetRandomSampler(valid_indexes)\n",
    "test_sampler = SubsetRandomSampler(test_indexes)\n",
    "\n",
    "# train loader (for calculating normalize parameters)\n",
    "loader = DataLoader(dataset=dataset, batch_size=Config.BATCH_SIZE, shuffle=False, sampler=train_sampler)\n",
    "\n",
    "# batch means and stds\n",
    "batch_means = []\n",
    "batch_stds = []\n",
    "for i, sample in enumerate(loader):\n",
    "    images = sample['image']    \n",
    "    batch_means.append(np.mean(images.numpy(), axis=(0,1,2))) # batch, height, width\n",
    "    batch_stds.append(np.std(images.numpy(), axis=(0,1,2), ddof=1)) # batch, height, width\n",
    "\n",
    "# overall mean and std per channel\n",
    "means = np.array(batch_means).mean(axis=0)\n",
    "stds = np.array(batch_stds).mean(axis=0)\n",
    "\n",
    "print(f'Means: {means}\\nStds:  {stds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "train_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    Normalize(mean=means, std=stds),\n",
    "    RandomChoice([\n",
    "        BrightnessJitter(brightness=.25),\n",
    "        ContrastJitter(contrast=.15),\n",
    "        SaturationJitter(saturation=.15),\n",
    "        HueJitter(hue=.1),\n",
    "        ]),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "val_test_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    Normalize(mean=means, std=stds),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "# datasets (using samplers from previous step to create train/valid/test split)\n",
    "train_dataset = LandCoverDataset(root_dir=Config.DATA_FOLDER, transform=train_transform)\n",
    "train_dataset = Subset(dataset=train_dataset, indices=train_sampler.indices)\n",
    "\n",
    "val_test_dataset = LandCoverDataset(root_dir=Config.DATA_FOLDER, transform=val_test_transform)\n",
    "valid_dataset = Subset(dataset=val_test_dataset, indices=valid_sampler.indices)\n",
    "test_dataset = Subset(dataset=val_test_dataset, indices=test_sampler.indices)\n",
    "\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}